# ğŸ™ï¸ Kavita â€” AI Voice Appointment Agent

A web-based AI voice agent with an animated avatar that conducts natural phone-style conversations to book, retrieve, cancel, and modify appointments in real time.

> `python main.py` â†’ open `http://localhost:8765` â†’ click **Start Conversation**

![Python](https://img.shields.io/badge/Python-3.10+-blue) ![OpenAI](https://img.shields.io/badge/LLM-GPT--4o--mini-green) ![Azure](https://img.shields.io/badge/STT-Azure%20Speech-0078D4) ![Cartesia](https://img.shields.io/badge/TTS-Cartesia-purple)

---

## âœ¨ What It Does

- **Listens** to what you say via Azure Speech-to-Text (real-time streaming)
- **Thinks** using OpenAI GPT-4o-mini with function-calling (8 appointment tools)
- **Speaks** back naturally via Cartesia TTS (WebSocket streaming)
- **Shows** an animated SVG avatar with real-time lip-sync, eye blinks, and breathing
- **Displays** every tool call live on the web UI (slot grids, appointment cards, user profiles)
- **Summarizes** the entire conversation at the end and shows it as an overlay

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            main.py                                  â”‚
â”‚       Creates IPC objects, spawns processes, runs event loop        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                     â”‚
          â–¼                    â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Speaker Process  â”‚  â”‚ Conversation Manager â”‚  â”‚   Avatar Server   â”‚
â”‚ (multiprocessing)â”‚  â”‚    (async task)      â”‚  â”‚    (aiohttp)      â”‚
â”‚                  â”‚  â”‚                      â”‚  â”‚                   â”‚
â”‚ â€¢ CartesiaTTS    â”‚  â”‚ â€¢ MicStream capture  â”‚  â”‚ â€¢ Serves HTML UI  â”‚
â”‚ â€¢ PyAudio play   â”‚  â”‚ â€¢ Azure STT (push)   â”‚  â”‚ â€¢ WebSocket push  â”‚
â”‚ â€¢ RMS energy     â”‚  â”‚ â€¢ LLM orchestration  â”‚  â”‚ â€¢ Start button    â”‚
â”‚   measurement    â”‚  â”‚ â€¢ Tool dispatch      â”‚  â”‚ â€¢ Event broadcast â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚         mp.Manager() Queues                   â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
         â”œâ”€â”€â”‚ mp_commands_queue        â”‚  (speak / terminate)
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”œâ”€â”€â”‚ agent_status_queue       â”‚  (speaking / done / interrupted)
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  â”‚ avatar_queue             â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Browser
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  (WebSocket)
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Cartesia  â”‚â”€â”€â”€â–ºâ”‚ PyAudio      â”‚    â”‚ data/            â”‚
   â”‚ TTS API   â”‚    â”‚ Speaker      â”‚    â”‚ appointments.jsonâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow (single utterance)

```
[Microphone]
    â”‚  audio chunks (16kHz, 16-bit, mono)
    â–¼
[MicStream] â”€â”€â–º raw PCM via run_in_executor (non-blocking)
    â”‚
    â–¼
[SimpleAzureSTT] â”€â”€â–º push_stream.write(chunk)
    â”‚                  Azure processes in cloud
    â”‚                  returns partial â†’ final transcriptions
    â–¼
[Conversation Manager] â”€â”€â–º collects partials, waits 2s silence
    â”‚                       joins fragments into full utterance
    â–¼
[ConversationalLLM] â”€â”€â–º OpenAI chat.completions with tool definitions
    â”‚                    LLM may call tools (identify_user, fetch_slots, etc.)
    â”‚                    tool results fed back â†’ LLM generates final text
    â–¼
[mp_commands_queue] â”€â”€â–º {"action": "speak", "text": "...", "turn_id": N}
    â”‚
    â–¼
[Speaker Process]
    â”‚  CartesiaTTS.stream(text) â†’ yields audio chunks
    â”‚  each chunk: measure RMS â†’ send audio_energy to avatar_queue
    â”‚  PyAudio stream.write(chunk) â†’ plays through speakers
    â”‚  agent_status_queue.put({"action": "done_speaking"})
    â–¼
[Avatar Server] â—„â”€â”€ avatar_queue events
    â”‚  broadcasts via WebSocket to all connected browsers
    â–¼
[Browser â€” index.html]
    â€¢ mouth opens/closes based on audio_energy
    â€¢ transcript updates with agent/user text
    â€¢ tool call cards render with structured results
```

---

## ğŸ“‚ Project Structure

```
voice_stack/
â”œâ”€â”€ main.py                          # Entry point â€” process orchestration
â”œâ”€â”€ mic_stream.py                    # Microphone capture (sounddevice)
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ conversation_manager.py      # Main async orchestrator
â”‚   â”œâ”€â”€ llm_handler.py               # OpenAI LLM with function-calling
â”‚   â””â”€â”€ speaker.py                   # TTS + playback (separate process)
â”‚
â”œâ”€â”€ stt/
â”‚   â””â”€â”€ azure_stt.py                 # Azure Speech-to-Text (push stream)
â”‚
â”œâ”€â”€ tts/
â”‚   â””â”€â”€ cartesia_tts.py              # Cartesia TTS (WebSocket streaming)
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ appointment_handler.py       # 8 tool functions + OpenAI tool schemas
â”‚   â””â”€â”€ conversation_summarizer.py   # Post-call LLM summary generation
â”‚
â”œâ”€â”€ avatar/
â”‚   â”œâ”€â”€ server.py                    # aiohttp WebSocket server
â”‚   â””â”€â”€ index.html                   # Avatar UI (SVG + tool cards + transcript)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py                    # Custom logger
â”‚   â”œâ”€â”€ conversation_logger.py       # Per-call turn logging
â”‚   â””â”€â”€ prompts.py                   # agent prompts
â”‚
â””â”€â”€ data/
    â””â”€â”€ appointments.json            # Appointment database (auto-created with seed data)
```

---

## ğŸš€ Quick Start

### 1. Create & activate virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate           # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Create your `.env` file

> âš ï¸ **No `.env.example` is included.** You must create the `.env` file yourself in the project root.

```bash
touch .env
```

Open `.env` in any editor and paste:

```env
# â”€â”€â”€ REQUIRED: Speech-to-Text (Azure) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AZURE_SPEECH_KEY=your_azure_speech_key_here
AZURE_SPEECH_REGION=southeastasia

# â”€â”€â”€ REQUIRED: LLM (OpenAI) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY=your_openai_api_key_here

# â”€â”€â”€ REQUIRED: Text-to-Speech (Cartesia) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CARTESIA_API_KEY=your_cartesia_api_key_here

# â”€â”€â”€ OPTIONAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AGENT_NAME=kavita                   # Avatar display name
OPENAI_MODEL=gpt-4o-mini           # LLM model to use
AVATAR_PORT=8765                    # Web UI port
```

Replace the placeholder values with your actual API keys (see section below for where to get them).

### 3. Run

```bash
python main.py
```

You will see:

```
[main] ğŸš€ Appointment Booking Agent started (call_id=abc12345)
[main] ğŸ”Š Speaker PID: 12345
[main] ğŸ­ Avatar: http://localhost:8765
[main] ğŸŒ Open the URL and click 'Start Conversation' to begin
```

Open **http://localhost:8765** in your browser â†’ click **Start Conversation** â†’ speak.
**Note:** Try to use earphones, since its mic stream, in case of speaker, agent voice will be sent back as the user.

---

## ğŸ”‘ Where to Get API Keys

| Service | What For | Free Tier | Sign Up |
|---|---|---|---|
| **Azure Speech** | Speech-to-Text | 5 hrs/month free | [portal.azure.com](https://portal.azure.com) â†’ Create a "Speech" resource |
| **OpenAI** | LLM + tool calling | Pay-as-you-go | [platform.openai.com](https://platform.openai.com) |
| **Cartesia** | Text-to-Speech | Check current limits | [cartesia.ai](https://cartesia.ai) |

---

## ğŸ® Usage

1. **Start** â€” Click the ğŸ™ï¸ button on the avatar page
2. **Identify** â€” Say your 4-digit user ID (e.g., *"my ID is 1001"*)
3. **Book** â€” *"I'd like to book an appointment on February 12th"*
4. **Check** â€” *"What appointments do I have?"*
5. **Cancel** â€” *"Cancel my appointment"*
6. **Modify** â€” *"Move my 10 AM to 2 PM"*
7. **End** â€” *"Bye"* or *"That's all"*

### Pre-loaded Test Users

| User ID | Name | Existing Appointments |
|---|---|---|
| `1001` | Archit Ojha | Feb 10 @ 10:00 AM, Feb 14 @ 2:30 PM |
| `2045` | Priya Sharma | Feb 12 @ 11:00 AM, Feb 10 @ 3:00 PM |
| `3300` | Rahul Verma | *(none â€” test empty state)* |
| `9999` | *(not registered)* | *(test new user registration flow)* |

---

## ğŸ”§ Tool Functions

| # | Tool | Trigger | What Happens |
|---|---|---|---|
| 1 | `identify_user` | User gives 4-digit ID | Looks up profile + active appointments |
| 2 | `register_user` | ID not found, user agrees | Generates new 4-digit ID, saves to DB |
| 3 | `fetch_slots` | Before any booking | Returns free 30-min slots (9 AM â€“ 6 PM) |
| 4 | `book_appointment` | User picks a slot | Books with double-booking prevention |
| 5 | `retrieve_appointments` | "Check my appointments" | Returns active + cancelled history |
| 6 | `cancel_appointment` | User wants to cancel | Soft-delete (preserves audit trail) |
| 7 | `modify_appointment` | "Move my appointment" | Changes date/time with conflict check |
| 8 | `end_conversation` | "Bye", "done", "that's all" | Ends call â†’ triggers summary |

Every tool call is **rendered live** on the web UI as a styled card with slot grids, appointment details, and status badges.

---

## ğŸ–¥ï¸ Web UI Panels

| Panel | Location | What It Shows |
|---|---|---|
| **Avatar** | Left | Animated SVG face â€” lip-sync, blinking, breathing |
| **User Card** | Left (below avatar) | Appears after `identify_user` â€” name, ID, active appointments |
| **Transcript** | Right top | Live conversation â€” agent (purple), user (green) |
| **Tool Calls** | Right bottom | Real-time tool cards with custom rendering per type |
| **Start Overlay** | Fullscreen | Blocks until user clicks Start â€” required for mic |
| **Summary Overlay** | Fullscreen | Appears at call end â€” LLM-generated conversation summary |

---

## âš¡ Performance

| Metric | Typical |
|---|---|
| STT first partial | ~200ms |
| LLM response | 0.5â€“1.5s |
| TTS first byte | ~300ms |
| **End-to-end latency** | **~1.5â€“2.5s** |
| Summary generation | 1â€“3s |
| Avatar lip-sync delay | <50ms |

---

## ğŸ§  Design Decisions

| Decision | Why |
|---|---|
| **Multiprocessing for speaker** | PyAudio's blocking `stream.write()` would freeze the async event loop |
| **`run_in_executor` for mic** | sounddevice's blocking iterator would starve aiohttp; thread pool keeps event loop free |
| **Soft-delete cancellations** | Preserves history; agent can answer "did I cancel something?"; slot is still freed |
| **SVG avatar** | Zero cost, zero API dependency, instant load; lip-sync driven by actual TTS audio RMS |
| **Start button required** | Browser autoplay policies need a user gesture; also prevents talking to empty rooms |
| **JSON file DB** | Simple, human-readable, zero dependencies; sufficient for demo |
| **max_tokens=100** | Forces LLM to keep voice responses short (~30 words) â€” critical for voice UX |
| **45s silence timeout** | Long enough for user to think after agent speaks; resets while agent is talking |

---

## ğŸ“‹ Known Limitations

- **Single user at a time** â€” one mic, one speaker, one conversation
- **JSON database** â€” not production-grade; no concurrent write safety
- **English-primary** â€” Azure STT configured for `en-US`
- **Local mic only** â€” no WebRTC; requires physical microphone access
- **Date edge cases** â€” LLM handles "tomorrow" and "next Monday" well, but ambiguous phrasing may vary
- **No auth** â€” any user ID can look up any appointments

---

## ğŸ“¦ Dependencies

```
openai                              # LLM with function-calling
azure-cognitiveservices-speech      # Speech-to-Text
cartesia                            # Text-to-Speech
aiohttp                             # Avatar WebSocket server
sounddevice                         # Microphone capture
pyaudio                             # Audio playback
numpy                               # Audio array handling
python-dotenv                       # .env loading
```

---
